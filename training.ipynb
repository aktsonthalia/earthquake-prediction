{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from preprocessing.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import math\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda = torch.cuda.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, loss_function, train_size=0.8, n_epochs=1, lookback_window=50):\n",
    "    \n",
    "    a, b, c = data.size()\n",
    "    \n",
    "    train_size = int(train_size * a) - lookback_window\n",
    "    test_size = a - train_size - lookback_window\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        \n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        \n",
    "        for i in tqdm(range(a - lookback_window)):\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            X = data[i : i+lookback_window]\n",
    "            y = data[i+lookback_window]\n",
    "\n",
    "            out = model(X)\n",
    "            loss = loss_function(out, y)\n",
    "                \n",
    "            if i < train_size:\n",
    "                \n",
    "                train_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                test_loss += loss\n",
    "            \n",
    "        print(\"Epoch {} completed. \\nTraining Loss: {:.3f}\".format(epoch, train_loss / (train_size)))\n",
    "        print(\"Test Loss: {:.3f}\".format(test_loss / (test_size)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0170, -0.0040,  0.2274,  0.2695, -0.0939,  0.2418,  0.2621,\n",
      "          -0.1861,  0.0557]]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_parameters, hidden_parameters, linear_parameters, out_parameters, num_layers=1, dropout=0.5, batch_size=1, activation=False):\n",
    "        \n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.rnn = nn.LSTM(in_parameters, hidden_parameters, num_layers)\n",
    "        self.linear1 = nn.Linear(hidden_parameters, linear_parameters)\n",
    "        self.linear2 = nn.Linear(linear_parameters, out_parameters)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.hidden_parameters = hidden_parameters\n",
    "        self.out_parameters = out_parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        self.hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_parameters)\n",
    "        self.cell = torch.zeros(self.num_layers, self.batch_size, self.hidden_parameters)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.hidden = self.hidden.cuda()\n",
    "            self.cell = self.cell.cuda()\n",
    "            \n",
    "    def forward(self, X):\n",
    "        \n",
    "        self.init_hidden()\n",
    "        x = torch.tensor((X.size()))\n",
    "        for i in range(X.size()[0]):\n",
    "            x, (self.hidden, self.cell) = self.rnn(X[i:i+1], (self.hidden, self.cell)) \n",
    "        X = x\n",
    "        X = X.view((1, -1))\n",
    "        X = self.linear1(X)\n",
    "        X = self.tanh(X)\n",
    "        X = self.linear2(X)\n",
    "        \n",
    "        \n",
    "        if self.activation:\n",
    "            X = self.activation(X)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# SAMPLE USAGE\n",
    "\n",
    "# model = LSTM(9, 20, 20, 9)\n",
    "# model = model.cuda()\n",
    "# X = torch.randn((50, 1, 9))\n",
    "# X = X.cuda()\n",
    "# print(model(X))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 9])\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, in_parameters, activation=None):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_parameters = in_parameters\n",
    "        self.conv_out_size = out_channels * (in_parameters - kernel_size + 1)**2\n",
    "        self.activation = activation\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_parameters, in_parameters*in_parameters)\n",
    "        self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.linear2 = nn.Linear(self.conv_out_size, in_parameters*in_parameters)\n",
    "        self.linear3 = nn.Linear(in_parameters*in_parameters, in_parameters)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.linear1(X)\n",
    "        X = X.view((1, self.in_channels, self.in_parameters, self.in_parameters))\n",
    "        X = self.tanh(X)\n",
    "        X = self.cnn(X)\n",
    "        X = X.view((1, -1))\n",
    "        X = self.linear2(X)\n",
    "        X = self.tanh(X)\n",
    "        X = self.linear3(X)\n",
    "        \n",
    "        if self.activation:\n",
    "            X = self.activation(X)\n",
    "            \n",
    "        \n",
    "        return X\n",
    "\n",
    "model = CNN(10, 16, 3, 9)\n",
    "\n",
    "X = torch.randn((10, 1, 9))\n",
    "print(X.size())\n",
    "\n",
    "X = model(X)\n",
    "print(X.size())\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 9])\n",
      "torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "class CNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, in_parameters, hidden_parameters, num_layers, batch_size=1, activation=None):\n",
    "        \n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_parameters = in_parameters\n",
    "        self.rnn_in = (in_parameters - kernel_size + 1)**2\n",
    "        self.conv_out_size = out_channels * (in_parameters - kernel_size + 1)**2\n",
    "        self.activation = activation\n",
    "        self.hidden_parameters = hidden_parameters\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_parameters, in_parameters*in_parameters)\n",
    "        self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.linear2 = nn.Linear(self.conv_out_size, in_parameters*in_parameters)\n",
    "        self.linear3 = nn.Linear(hidden_parameters, in_parameters)\n",
    "        self.rnn = nn.LSTM(self.rnn_in, hidden_parameters, num_layers)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "       \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        self.hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_parameters)\n",
    "        self.cell = torch.zeros(self.num_layers, self.batch_size, self.hidden_parameters)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.hidden = self.hidden.cuda()\n",
    "            self.cell = self.cell.cuda()\n",
    "                \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.linear1(X)\n",
    "        X = X.view((1, self.in_channels, self.in_parameters, self.in_parameters))\n",
    "        X = self.cnn(X)\n",
    "        X = X[0]\n",
    "        X = X.view((X.size()[0], 1, -1))\n",
    "        X = self.relu(X)\n",
    "        X, _ = self.rnn(X)\n",
    "        X = X[-1]\n",
    "        X = self.relu(X)\n",
    "        X = self.linear3(X)\n",
    "        \n",
    "        if self.activation:\n",
    "            \n",
    "            X = self.activation(X)\n",
    "            \n",
    "        return X\n",
    "\n",
    "\n",
    "model = CNN_LSTM(in_channels=10, out_channels=16, kernel_size=3, in_parameters=9, hidden_parameters=50, num_layers=2)\n",
    "\n",
    "X = torch.randn((10, 1, 9))\n",
    "print(X.size())\n",
    "\n",
    "X = model(X)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 9])\n",
      "torch.Size([1, 90])\n"
     ]
    }
   ],
   "source": [
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_params, hidden1, hidden2, out_params):\n",
    "        \n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_params, hidden1)\n",
    "        self.linear2 = nn.Linear(hidden1, hidden2)\n",
    "        self.linear3 = nn.Linear(hidden2, out_params)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = X.view(-1)\n",
    "        X = self.linear1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.linear2(X)\n",
    "        X = self.relu(X)        \n",
    "        X = self.linear3(X)\n",
    "        X = self.tanh(X)\n",
    "        X = X.view((1 ,-1))\n",
    "        \n",
    "        return X\n",
    "    \n",
    "model = ANN(90, 100, 200, 90)\n",
    "\n",
    "X = torch.randn((10, 1, 9))\n",
    "print(X.size())\n",
    "\n",
    "X = model(X)\n",
    "print(X.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
